{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with H2O - Tutorial 4b: Classification Models (Ensembles)\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Objective**:\n",
    "\n",
    "- This tutorial explains how to create stacked ensembles of classification models for better out-of-bag performance.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Titanic Dataset:**\n",
    "\n",
    "- Source: https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "<hr>\n",
    "    \n",
    "**Steps**:\n",
    "\n",
    "1. Build GBM models using random grid search and extract the best one.\n",
    "2. Build DRF models using random grid search and extract the best one. \n",
    "3. Use model stacking to combining different models.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Full Technical Reference:**\n",
    "\n",
    "- http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html\n",
    "- http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"1.8.0_121\"; Java(TM) SE Runtime Environment (build 1.8.0_121-b13); Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)\n",
      "  Starting server from /home/joe/anaconda3/lib/python3.5/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpx851xfau\n",
      "  JVM stdout: /tmp/tmpx851xfau/h2o_joe_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpx851xfau/h2o_joe_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.10.3.5</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>9 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_joe_nhkkxm</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>5.210 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.5.2 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster version:        3.10.3.5\n",
       "H2O cluster version age:    9 days\n",
       "H2O cluster name:           H2O_from_python_joe_nhkkxm\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    5.210 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "Python version:             3.5.2 final\n",
       "--------------------------  ------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import all required modules\n",
    "import h2o\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "# Start and connect to a local H2O cluster\n",
    "h2o.init(nthreads = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  PassengerId</th><th style=\"text-align: right;\">  Survived</th><th style=\"text-align: right;\">  Pclass</th><th>Name                                               </th><th>Sex   </th><th style=\"text-align: right;\">  Age</th><th style=\"text-align: right;\">  SibSp</th><th style=\"text-align: right;\">  Parch</th><th style=\"text-align: right;\">  Ticket</th><th style=\"text-align: right;\">   Fare</th><th>Cabin  </th><th>Embarked  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">       3</td><td>Braund, Mr. Owen Harris                            </td><td>male  </td><td style=\"text-align: right;\">   22</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\"> 7.25  </td><td>       </td><td>S         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">       1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td style=\"text-align: right;\">   38</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">71.2833</td><td>C85    </td><td>C         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">       3</td><td>Heikkinen, Miss. Laina                             </td><td>female</td><td style=\"text-align: right;\">   26</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\"> 7.925 </td><td>       </td><td>S         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">       1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)       </td><td>female</td><td style=\"text-align: right;\">   35</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">  113803</td><td style=\"text-align: right;\">53.1   </td><td>C123   </td><td>S         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">       3</td><td>Allen, Mr. William Henry                           </td><td>male  </td><td style=\"text-align: right;\">   35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">  373450</td><td style=\"text-align: right;\"> 8.05  </td><td>       </td><td>S         </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Titanic data (local CSV)\n",
    "titanic = h2o.import_file(\"kaggle_titanic.csv\")\n",
    "titanic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert 'Survived' and 'Pclass' to categorical values\n",
    "titanic['Survived'] = titanic['Survived'].asfactor()\n",
    "titanic['Pclass'] = titanic['Pclass'].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define features (or predictors) manually\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the H2O data frame into training/test sets\n",
    "# so we can evaluate out-of-bag performance\n",
    "titanic_split = titanic.split_frame(ratios = [0.8], seed = 1234)\n",
    "\n",
    "titanic_train = titanic_split[0] # using 80% for training\n",
    "titanic_test = titanic_split[1]  # using the rest 20% for out-of-bag evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Search Criteria for Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the criteria for random grid search\n",
    "search_criteria = {'strategy': \"RandomDiscrete\", \n",
    "                   'max_models': 9,\n",
    "                   'seed': 1234}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build GBM Models using Random Grid Search and Extract the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the range of hyper-parameters for GBM grid search\n",
    "# 27 combinations in total\n",
    "hyper_params = {'sample_rate': [0.7, 0.8, 0.9],\n",
    "                'col_sample_rate': [0.7, 0.8, 0.9],\n",
    "                'max_depth': [3, 5, 7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up GBM grid search\n",
    "# Add a seed for reproducibility\n",
    "gbm_rand_grid = H2OGridSearch(\n",
    "                    H2OGradientBoostingEstimator(\n",
    "                        model_id = 'gbm_rand_grid', \n",
    "                        seed = 1234,\n",
    "                        ntrees = 10000,   \n",
    "                        nfolds = 5,\n",
    "                        fold_assignment = \"Modulo\",               # needed for stacked ensembles\n",
    "                        keep_cross_validation_predictions = True, # needed for stacked ensembles\n",
    "                        stopping_metric = 'mse', \n",
    "                        stopping_rounds = 15,     \n",
    "                        score_tree_interval = 1),\n",
    "                    search_criteria = search_criteria, # full grid search\n",
    "                    hyper_params = hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Grid Build progress: |████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Use .train() to start the grid search\n",
    "gbm_rand_grid.train(x = features, \n",
    "                    y = 'Survived', \n",
    "                    training_frame = titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    col_sample_rate max_depth sample_rate  \\\n",
      "0               0.8         3         0.8   \n",
      "1               0.9         3         0.9   \n",
      "2               0.8         3         0.9   \n",
      "3               0.7         3         0.7   \n",
      "4               0.9         7         0.7   \n",
      "5               0.7         5         0.8   \n",
      "6               0.7         7         0.7   \n",
      "7               0.8         7         0.7   \n",
      "8               0.9         7         0.9   \n",
      "\n",
      "                                                     model_ids  \\\n",
      "0  Grid_GBM_py_6_sid_b9ee_model_python_1488233279563_1_model_3   \n",
      "1  Grid_GBM_py_6_sid_b9ee_model_python_1488233279563_1_model_2   \n",
      "2  Grid_GBM_py_6_sid_b9ee_model_python_1488233279563_1_model_7   \n",
      "3  Grid_GBM_py_6_sid_b9ee_model_python_1488233279563_1_model_8   \n",
      "4  Grid_GBM_py_6_sid_b9ee_model_python_1488233279563_1_model_6   \n",
      "5  Grid_GBM_py_6_sid_b9ee_model_python_1488233279563_1_model_0   \n",
      "6  Grid_GBM_py_6_sid_b9ee_model_python_1488233279563_1_model_1   \n",
      "7  Grid_GBM_py_6_sid_b9ee_model_python_1488233279563_1_model_4   \n",
      "8  Grid_GBM_py_6_sid_b9ee_model_python_1488233279563_1_model_5   \n",
      "\n",
      "                  auc  \n",
      "0   0.866063061922249  \n",
      "1  0.8641045122620403  \n",
      "2  0.8622050567726142  \n",
      "3  0.8610653834789583  \n",
      "4  0.8580515807690684  \n",
      "5  0.8557004769743785  \n",
      "6  0.8542568908024145  \n",
      "7  0.8530665653623739  \n",
      "8  0.8423325313410156  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort and show the grid search results\n",
    "gbm_rand_grid_sorted = gbm_rand_grid.get_grid(sort_by='auc', decreasing=True)\n",
    "print(gbm_rand_grid_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>number_of_internal_trees</b></td>\n",
       "<td><b>model_size_in_bytes</b></td>\n",
       "<td><b>min_depth</b></td>\n",
       "<td><b>max_depth</b></td>\n",
       "<td><b>mean_depth</b></td>\n",
       "<td><b>min_leaves</b></td>\n",
       "<td><b>max_leaves</b></td>\n",
       "<td><b>mean_leaves</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>74.0</td>\n",
       "<td>74.0</td>\n",
       "<td>14531.0</td>\n",
       "<td>3.0</td>\n",
       "<td>3.0</td>\n",
       "<td>3.0</td>\n",
       "<td>4.0</td>\n",
       "<td>8.0</td>\n",
       "<td>6.918919</td></tr></table></div>"
      ],
      "text/plain": [
       "    number_of_trees    number_of_internal_trees    model_size_in_bytes    min_depth    max_depth    mean_depth    min_leaves    max_leaves    mean_leaves\n",
       "--  -----------------  --------------------------  ---------------------  -----------  -----------  ------------  ------------  ------------  -------------\n",
       "    74                 74                          14531                  3            3            3             4             8             6.91892"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the best model from random grid search\n",
    "best_gbm_model_id = gbm_rand_grid_sorted.model_ids[0]\n",
    "best_gbm_from_rand_grid = h2o.get_model(best_gbm_model_id)\n",
    "best_gbm_from_rand_grid.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build DRF Models using Random Grid Search and Extract the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the range of hyper-parameters for DRF grid search\n",
    "# 27 combinations in total\n",
    "hyper_params = {'sample_rate': [0.5, 0.6, 0.7],\n",
    "                'col_sample_rate_per_tree': [0.7, 0.8, 0.9],\n",
    "                'max_depth': [3, 5, 7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up DRF grid search\n",
    "# Add a seed for reproducibility\n",
    "drf_rand_grid = H2OGridSearch(\n",
    "                    H2ORandomForestEstimator(\n",
    "                        model_id = 'drf_rand_grid', \n",
    "                        seed = 1234,\n",
    "                        ntrees = 200,   \n",
    "                        nfolds = 5,\n",
    "                        fold_assignment = \"Modulo\",                # needed for stacked ensembles\n",
    "                        keep_cross_validation_predictions = True), # needed for stacked ensembles\n",
    "                    search_criteria = search_criteria, # full grid search\n",
    "                    hyper_params = hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Grid Build progress: |████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Use .train() to start the grid search\n",
    "drf_rand_grid.train(x = features, \n",
    "                    y = 'Survived', \n",
    "                    training_frame = titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    col_sample_rate_per_tree max_depth sample_rate  \\\n",
      "0                        0.8         7         0.5   \n",
      "1                        0.9         7         0.5   \n",
      "2                        0.9         7         0.7   \n",
      "3                        0.7         7         0.5   \n",
      "4                        0.7         5         0.6   \n",
      "5                        0.8         3         0.7   \n",
      "6                        0.8         3         0.6   \n",
      "7                        0.7         3         0.5   \n",
      "8                        0.9         3         0.7   \n",
      "\n",
      "                                                         model_ids  \\\n",
      "0  Grid_DRF_py_6_sid_b9ee_model_python_1488233279563_13356_model_4   \n",
      "1  Grid_DRF_py_6_sid_b9ee_model_python_1488233279563_13356_model_6   \n",
      "2  Grid_DRF_py_6_sid_b9ee_model_python_1488233279563_13356_model_5   \n",
      "3  Grid_DRF_py_6_sid_b9ee_model_python_1488233279563_13356_model_1   \n",
      "4  Grid_DRF_py_6_sid_b9ee_model_python_1488233279563_13356_model_0   \n",
      "5  Grid_DRF_py_6_sid_b9ee_model_python_1488233279563_13356_model_7   \n",
      "6  Grid_DRF_py_6_sid_b9ee_model_python_1488233279563_13356_model_3   \n",
      "7  Grid_DRF_py_6_sid_b9ee_model_python_1488233279563_13356_model_8   \n",
      "8  Grid_DRF_py_6_sid_b9ee_model_python_1488233279563_13356_model_2   \n",
      "\n",
      "                  auc  \n",
      "0  0.8625469587607109  \n",
      "1  0.8618504917479212  \n",
      "2  0.8604828837955342  \n",
      "3  0.8603182643197839  \n",
      "4  0.8588240260014351  \n",
      "5  0.8546536659490945  \n",
      "6   0.854248448778017  \n",
      "7   0.853218521801528  \n",
      "8  0.8494998100544511  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort and show the grid search results\n",
    "drf_rand_grid_sorted = drf_rand_grid.get_grid(sort_by='auc', decreasing=True)\n",
    "print(drf_rand_grid_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>number_of_internal_trees</b></td>\n",
       "<td><b>model_size_in_bytes</b></td>\n",
       "<td><b>min_depth</b></td>\n",
       "<td><b>max_depth</b></td>\n",
       "<td><b>mean_depth</b></td>\n",
       "<td><b>min_leaves</b></td>\n",
       "<td><b>max_leaves</b></td>\n",
       "<td><b>mean_leaves</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>200.0</td>\n",
       "<td>200.0</td>\n",
       "<td>125028.0</td>\n",
       "<td>7.0</td>\n",
       "<td>7.0</td>\n",
       "<td>7.0</td>\n",
       "<td>24.0</td>\n",
       "<td>61.0</td>\n",
       "<td>41.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    number_of_trees    number_of_internal_trees    model_size_in_bytes    min_depth    max_depth    mean_depth    min_leaves    max_leaves    mean_leaves\n",
       "--  -----------------  --------------------------  ---------------------  -----------  -----------  ------------  ------------  ------------  -------------\n",
       "    200                200                         125028                 7            7            7             24            61            41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the best model from random grid search\n",
    "best_drf_model_id = drf_rand_grid_sorted.model_ids[0]\n",
    "best_drf_from_rand_grid = h2o.get_model(best_drf_model_id)\n",
    "best_drf_from_rand_grid.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a list of models to be stacked\n",
    "# i.e. best model from each grid\n",
    "all_ids = [best_gbm_model_id, best_drf_model_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up Stacked Ensemble\n",
    "ensemble = H2OStackedEnsembleEstimator(model_id = \"my_ensemble\",\n",
    "                                       base_models = all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble Model Build progress: |███████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# use .train to start model stacking\n",
    "# GLM as the default metalearner\n",
    "ensemble.train(x = features, \n",
    "               y = 'Survived', \n",
    "               training_frame = titanic_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Model Performance on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GBM model from Grid (AUC) :  0.8892284186401833\n",
      "Best DRF model from Grid (AUC) :  0.8903106697224344\n",
      "Stacked Ensembles        (AUC) :  0.8918385536032595\n"
     ]
    }
   ],
   "source": [
    "print('Best GBM model from Grid (AUC) : ', best_gbm_from_rand_grid.model_performance(titanic_test).auc())\n",
    "print('Best DRF model from Grid (AUC) : ', best_drf_from_rand_grid.model_performance(titanic_test).auc())\n",
    "print('Stacked Ensembles        (AUC) : ', ensemble.model_performance(titanic_test).auc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
